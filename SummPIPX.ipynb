{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7320b731-8734-4fa3-9a3d-f3c0b0ff3e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3e0ed9-ab75-4f0b-a56d-4fd13d206830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86013c39-32d3-4c44-8c01-b7ba75f41657",
   "metadata": {},
   "source": [
    "## load_documents:\n",
    "- Input:\n",
    "    - input_data: list of strings (documents) or list of file paths (strings)\n",
    "    - input_type: string ('documents' or 'file_paths')\n",
    "- Output: \n",
    "    - list of strings (documents)\n",
    "\n",
    "## build_sentence_graph:\n",
    "- Input: \n",
    "    - list of strings (documents)\n",
    "- Output: \n",
    "    - 2D list (adjacency list representing the sentence graph)\n",
    "\n",
    "## spectral_clustering:\n",
    "- Input: \n",
    "    - 2D list (adjacency list representing the sentence graph)\n",
    "- Output: \n",
    "    - list of lists, where each inner list contains spaCy sentence objects (each cluster)\n",
    "\n",
    "## fit:\n",
    "- Input: \n",
    "    - list of strings (documents)  \n",
    "    \n",
    "> No output (updates the self.clusters attribute)\n",
    "\n",
    "## compress_clusters:\n",
    "- Input: \n",
    "    - list of lists, where each inner list contains spaCy sentence objects (clusters)\n",
    "- Output: \n",
    "    - string (final summary)\n",
    "\n",
    "## transform:\n",
    "- Output: \n",
    "    - string (final summary)  \n",
    "    \n",
    "> No input (uses the self.clusters attribute)\n",
    "\n",
    "## fit_transform:\n",
    "- Input: \n",
    "    - list of strings (documents)\n",
    "    - Output: string (final summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77ff9e-2942-45fc-a35f-06045f068667",
   "metadata": {},
   "source": [
    "## Sentence graph module from SummPIP Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb9f1af-4abf-4ef9-a603-b0ea8b4c5874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2020-01-16 10:17 PM\n",
    "\n",
    "author  : michelle\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from ordered_set import OrderedSet\n",
    "import scipy\n",
    "from scipy import *\n",
    "\n",
    "\n",
    "glove_word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "spacynlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 1: Deverbal Noun Reference\n",
    "# step 1.1: get nouns for verbs in the current sentence\n",
    "verbs_to_escape = [\"be\", \"is\",\"am\",\"are\",\"was\", \"were\", \"being\",\"been\",\"do\",\"did\",\n",
    "               \"done\",\"have\",\"had\",\"get\",\"got\",\"gotten\"]\n",
    "\n",
    "# Step 3: Discourse Markers => only for two adjacent sentences\n",
    "markers=[\"for\",\"so\",\"because\",\"since\",\"therefore\",\"consequently\",\"additionally\",\"furthermore\",\"moreover\",\n",
    "         \"but\",\"however\",\"although\",\"despite\",\"similarly\",\"otherwise\",\"whereas\",\"while\",\"unlike\",\"thus\",\n",
    "        \"instead\",\"nevertheless\",\"afterward\",\"finally\",\"subsequently\",\"conversely\",\"later\",\"next\",\"then\",\n",
    "         \"likewise\",\"compared\",\"besides\",\"further\",\"as\",\"also\",\"equally\",\"hence\",\"accordingly\",\"stil\",\n",
    "        \"simultaneously\"]\n",
    "# 39 markers\n",
    "\n",
    "class SentenceGraph:\n",
    "    def __init__(self, sentences_list, w2v, use_lm, lm_model, lm_tokenizer, ita=0.9, threshold=0.65):\n",
    "        self.sentences_list = sentences_list\n",
    "\n",
    "        self.length = len(sentences_list)\n",
    "\n",
    "        self.w2v = w2v\n",
    "\n",
    "        self.use_lm = use_lm \n",
    "\n",
    "        self.lm_model = lm_model\n",
    "\n",
    "        self.tokenizer = lm_tokenizer\n",
    "\n",
    "        # threshold for step1\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # threshold for step4\n",
    "        self.ita = ita\n",
    "\n",
    "    def get_nouns_for_verbs(self, string):\n",
    "        doc = spacynlp(string)\n",
    "        nouns_list = []\n",
    "        if len(doc)>0:\n",
    "            for token in doc:\n",
    "                # find noun reference for verbs, escaping verbs that are too ambiguous\n",
    "                if token.pos_ == \"VERB\" and token.text not in verbs_to_escape:\n",
    "                    # print(\"token.text \", token.text)\n",
    "                    noun_forms = self._nounify(token.text)\n",
    "                    nouns_list.extend(noun_forms)\n",
    "        return nouns_list\n",
    "\n",
    "    def _nounify(self, verb):\n",
    "        # get the lemmas of base verbs;\n",
    "        base = wn.morphy(verb, wn.VERB)\n",
    "        if base:\n",
    "            lemmas = wn.lemmas(base, pos=\"v\")\n",
    "            noun_forms = []\n",
    "            # derive noun forms for each lemma\n",
    "            for lemma in lemmas:\n",
    "                nouns = [forms.name() for forms in lemma.derivationally_related_forms()]\n",
    "                noun_forms.extend(nouns)\n",
    "            # remove repetition\n",
    "            nouns_set = OrderedSet(noun_forms)\n",
    "            return nouns_set\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # step 1.2: find most similar word from word2vec\n",
    "    # get most similar words for nouns, including itself\n",
    "    def find_most_similar_words(self, word_vectors,nouns_list,threshold=0.65):\n",
    "        similar_nouns_list=[]\n",
    "        nouns_list=list(set(nouns_list))\n",
    "        for noun in nouns_list:\n",
    "            try:\n",
    "                nn = word_vectors.most_similar(positive=[noun])\n",
    "                # keep nn whose have high similary score\n",
    "                nn = [ tuple_[0] for tuple_ in nn if tuple_[1] > threshold]\n",
    "                similar_nouns_list.extend(nn)\n",
    "            # pass on uncommon words\n",
    "            except KeyError:\n",
    "                pass\n",
    "        similar_nouns_list.extend(nouns_list)\n",
    "        return list(set(similar_nouns_list))\n",
    "\n",
    "    # check if deverbal noun reference exits in the subsequent sentence\n",
    "    def check_noun_reference(self, similar_nouns_list, subsequent_sen):\n",
    "        flag=False\n",
    "        doc = spacynlp(subsequent_sen)\n",
    "        if len(doc)>0:\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    if token.text in similar_nouns_list:\n",
    "                        flag=True\n",
    "                        break\n",
    "        return flag\n",
    "\n",
    "    # step 2: Event/Entity Continuation\n",
    "    # Str needs to be raw, i.e., use str before normalisation and stemming\n",
    "    def compare_name_entity(self, str1, str2):\n",
    "        flag = False\n",
    "        doc1 = spacynlp(str1)\n",
    "        doc2 = spacynlp(str2)\n",
    "        if len(doc1)>0 and len(doc2)>0:\n",
    "            ent_list1=[(ent.text, ent.label_) for ent in doc1.ents]\n",
    "            ent_list2=[(ent.text, ent.label_) for ent in doc2.ents]\n",
    "            for (text, label) in ent_list1:\n",
    "                if (text, label) in ent_list2:\n",
    "                    flag=True\n",
    "                    break\n",
    "        return flag\n",
    "\n",
    "    def check_discourse_markers(self, str1,str2):\n",
    "        flag = False\n",
    "        doc2 = spacynlp(str2)\n",
    "        if len(doc2)>0:\n",
    "            first_token = doc2[0].text\n",
    "            if first_token.lower() in markers:\n",
    "                flag = True\n",
    "        return flag\n",
    "\n",
    "    # compute the cos similarity between a and b. a, b are numpy arrays\n",
    "    def cos_sim(self, a, b):\n",
    "        return 1 - scipy.spatial.distance.cosine(a,b)\n",
    "\n",
    "\n",
    "    def make_graph_undirected(self, source, target, weight):\n",
    "        source.extend(target)\n",
    "        target.extend(source)\n",
    "        weight.extend(weight)\n",
    "        triplet_list=[ (source[i],target[i],weight[i]) for i in range(len(source))]\n",
    "        sorted_by_src = sorted(triplet_list, key=lambda x: (x[0],x[1]))\n",
    "\n",
    "        sorted_source = []\n",
    "        sorted_target = []\n",
    "        sorted_weight = []\n",
    "        for triplet in sorted_by_src:\n",
    "            sorted_source.append(triplet[0])\n",
    "            sorted_target.append(triplet[1])\n",
    "            sorted_weight.append(triplet[2])\n",
    "        return sorted_source, sorted_target, sorted_weight\n",
    "\n",
    "    # Step4: calculate sentence embeddings\n",
    "    def get_sentence_embeddings(self,string):\n",
    "        if not self.use_lm:\n",
    "            v = self.get_wv_embedding(string)\n",
    "        else:\n",
    "            v = self.get_lm_embedding(string)\n",
    "        return v\n",
    "\n",
    "    # get sentence embeddings with w2v\n",
    "    def get_wv_embedding(self, string):\n",
    "        word_embeddings = self.w2v\n",
    "        sent = string.lower()\n",
    "        eps = 1e-10\n",
    "        if len(sent) != 0:\n",
    "            vectors = [word_embeddings.get(w, np.zeros((100,))) for w in sent.split()]\n",
    "            v = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        v = v + eps\n",
    "        return v\n",
    "\n",
    "    # get language model embedding\n",
    "    def get_lm_embedding(self, string):\n",
    "        sent = string.lower()\n",
    "        eps = 1e-10\n",
    "        if len(sent)!= 0:\n",
    "            input_ids = torch.tensor([self.tokenizer.encode(sent)])\n",
    "            last_hidden_state = self.lm_model(input_ids)[0]\n",
    "            hidden_state=last_hidden_state.tolist()\n",
    "            v = np.mean(hidden_state,axis=1)\n",
    "        else:\n",
    "            v = np.zeros((768,))\n",
    "        v = v + eps\n",
    "        return v\n",
    "\n",
    "    # step 4: compare sentence similarity\n",
    "    def check_if_similar_sentences(self,sentence_emb1,sentence_emb2):\n",
    "        flag = False\n",
    "        similarity = self.cos_sim(sentence_emb1,sentence_emb2)\n",
    "        if similarity > self.ita:\n",
    "            flag = True\n",
    "        return flag\n",
    "\n",
    "\n",
    "    def build_sentence_graph(self,):\n",
    "        # spectral clustering  \n",
    "        X = np.zeros([self.length, self.length])\n",
    "        \n",
    "        # get the vector size\n",
    "        self.size = len(self.get_sentence_embeddings(self.sentences_list[0]))\n",
    "\n",
    "        # get sentence vector holder\n",
    "        emb_sentence_vectors = np.zeros([self.length,self.size])\n",
    " \n",
    "        for i in range(self.length):\n",
    "             emb_sen = self.get_sentence_embeddings(self.sentences_list[i])\n",
    "             emb_sentence_vectors[i,] = emb_sen\n",
    "        \n",
    "        # iterate all sentence nodes to check if they should be connected\n",
    "        for i in range(self.length):\n",
    "            flag = False\n",
    "            sen_i = self.sentences_list[i]\n",
    "            # check above steps\n",
    "            for j in range(i+1,self.length):\n",
    "                sen_j = self.sentences_list[j]\n",
    "                if (j-i) == 1:\n",
    "                    # perform step1 and step3,which are only for adjacent sentences\n",
    "                    nouns_list = self.get_nouns_for_verbs(sen_i)\n",
    "                    # get most similar words for above nouns, including itself\n",
    "                    similar_nouns_list = self.find_most_similar_words(glove_word_vectors, nouns_list,self.threshold)\n",
    "                    # check for devebal noun \n",
    "                    flag = self.check_noun_reference(similar_nouns_list, sen_j)\n",
    "                    if not flag:\n",
    "                        # check for disourse markers\n",
    "                        flag=self.check_discourse_markers(sen_i,sen_j)\n",
    "                else:\n",
    "                    # check for name entities\n",
    "                    flag=self.compare_name_entity(sen_i,sen_j)\n",
    "\n",
    "               # => step4 check for similar sentences\n",
    "                if not flag:\n",
    "                    # continue\n",
    "                    i_sen_emb = emb_sentence_vectors[i,]\n",
    "                    j_sen_emb = emb_sentence_vectors[j,]\n",
    "                    flag = self.check_if_similar_sentences(i_sen_emb,j_sen_emb)\n",
    "\n",
    "                if flag:                                      \n",
    "                    X[i,j] = 1\n",
    "                    X[j,i] = 1\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb24e55-d2c7-4710-967e-1bccaafe3b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import spacy\n",
    "\n",
    "class SummPIPX:\n",
    "    def __init__(self, clustering_method='spectral_clustering', graph_method='build_sentence_graph', compression_method='compress_clusters'):\n",
    "        self.version = 'v1'\n",
    "        self.clustering_method = clustering_method\n",
    "        self.graph_method = graph_method\n",
    "        self.compression_method = compression_method\n",
    "\n",
    "    def __pdf_to_text__(self, path):\n",
    "        pdfreader = PyPDF2.PdfReader(path)\n",
    "        text=''\n",
    "        for page in pdfreader.pages:\n",
    "            text+=page.extract_text()\n",
    "        return text\n",
    "    \n",
    "    def load_documents(self, input_data, input_type='documents'):\n",
    "        # Check the input type\n",
    "        if input_type == 'documents':\n",
    "            # If input_type is 'documents', assume input_data represents the documents\n",
    "            documents = input_data\n",
    "        elif input_type == 'file_paths':\n",
    "            # If input_type is 'file_paths', assume input_data represents file paths\n",
    "            documents = [self.__pdf_to_text__(file) for file in files]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input_type: {input_type}\")\n",
    "        return documents\n",
    "    \n",
    "    def build_sentence_graph(self, documents):\n",
    "        # Default graph building method\n",
    "        pass\n",
    "\n",
    "    def build_another_graph_method(self, documents):\n",
    "        # Another graph building method\n",
    "        pass\n",
    "\n",
    "    def spectral_clustering(self, sentence_graph):\n",
    "        # Default clustering method\n",
    "        return []\n",
    "\n",
    "    def another_clustering_method(self, sentence_graph):\n",
    "        # Another clustering method\n",
    "        return []\n",
    "\n",
    "    def compress_clusters(self, clusters):\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "        summary_sentences = []\n",
    "\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) == 1:\n",
    "                # If there's only one sentence in the cluster, add it to the summary\n",
    "                summary_sentences.append(cluster[0])\n",
    "            else:\n",
    "                # Calculate the similarity scores between all sentences in the cluster\n",
    "                similarity_matrix = [[sent1.similarity(sent2) for sent2 in cluster] for sent1 in cluster]\n",
    "\n",
    "                # Calculate the sum of similarity scores for each sentence\n",
    "                similarity_sums = [sum(row) for row in similarity_matrix]\n",
    "\n",
    "                # Find the index of the sentence with the highest similarity score sum\n",
    "                most_relevant_index = similarity_sums.index(max(similarity_sums))\n",
    "\n",
    "                # Add the most relevant sentence to the summary\n",
    "                summary_sentences.append(cluster[most_relevant_index])\n",
    "\n",
    "        # Concatenate the summary sentences\n",
    "        summary = \" \".join(summary_sentences)\n",
    "        return summary\n",
    "\n",
    "    def another_compression_method(self, clusters):\n",
    "        # Another compression method\n",
    "        # ...\n",
    "        return ''\n",
    "\n",
    "    def fit(self, documents):\n",
    "        # Call the appropriate graph building method\n",
    "        graph_method = getattr(self, self.graph_method)\n",
    "        sentence_graph = graph_method(documents)\n",
    "        \n",
    "        # Call the appropriate clustering method\n",
    "        clustering_method = getattr(self, self.clustering_method)\n",
    "        self.clusters = clustering_method(sentence_graph)\n",
    "\n",
    "    def transform(self):\n",
    "        # Call the appropriate compression method\n",
    "        compression_method = getattr(self, self.compression_method)\n",
    "        summary = compression_method(self.clusters)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cc3ee-facd-4b66-b1e9-e8b606c17177",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae544837-bb62-4291-8390-75b2774c936e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112879"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=''\n",
    "with open('dataset/NeuralNetworks/mergeddoc.txt', 'r') as f:\n",
    "    text = '\\n'.join(f.readlines())\n",
    "\n",
    "graph = SentenceGraph(sentences_list, self.w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26ce2f78-87a1-448a-a46b-4a74a349336c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [\n",
    "    'dataset/NeuralNetworks/1460210.pdf', \n",
    "    'dataset/NeuralNetworks/Oken.pdf',\n",
    "    'dataset/NeuralNetworks/week7b-neuralnetwork.pdf'\n",
    "]\n",
    "\n",
    "summpip = SummPIPX()\n",
    "documents = summpip.load_documents(files, input_type='file_paths')\n",
    "summpip.fit(documents)\n",
    "summpip.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be04211-5084-4800-ab25-eb74d778fc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SummPIP-Kernel",
   "language": "python",
   "name": "summpip-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
