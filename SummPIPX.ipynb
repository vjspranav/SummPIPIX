{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f794d2-b2d2-45d8-8f6e-f0b4add20da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jyotheeswar  Project1  SummPip\tUntitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b442ab95-b279-4cd6-87b0-553c9d3d4591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOP Eyes Gains As Voters In 11 States Pick Governors Enlarge this image toggle caption Jim Cole / AP Jim Cole / AP Voters in 11 states will pick their governors tonight , and Republicans appear on track to increase their numbers by at least one , with the potential to extend their hold to more than two - thirds of the nation 's top state offices . Eight of the gubernatorial seats up for grabs are now held by Democrats ; three are in Republican hands . Republicans currently hold 29 governorships , Democrats have 20 , and Rhode Island 's Gov. Lincoln Chafee is an Independent . Polls and race analysts suggest that only three of tonight 's contests are considered competitive , all in states where incumbent Democratic governors are n't running again : Montana , New Hampshire and Washington . While those state races remain too close to call , Republicans are expected to wrest the North Carolina governorship from Democratic control , and to easily win GOP - held seats in Utah , North Dakota and Indiana . Democrats are likely to hold on to their seats in West Virginia and Missouri , and are expected to notch safe wins in races for seats they hold in Vermont and Delaware . Holding Sway On Health Care While the occupant of the governor 's office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to wield significant influence in at least one major area . And that 's health care , says political scientist Thad Kousser , co - author of The Power of American Governors . \" No matter who wins the story_separator_special_tag GOP Eyes Gains As Voters In 11 States Pick Governors Jim Cole / AP i Jim Cole / AP Voters in 11 states will pick their governors tonight , and Republicans appear on track to increase their numbers by at least one , and with the potential to extend their hold to more than two - thirds of the nation 's top state offices . Eight of the gubernatorial seats up for grabs today are now held by Democrats ; three are in Republican hands . Republicans currently hold 29 governorships , Democrats have 20 ; and Rhode Island 's Gov. Lincoln Chafee is an Independent . Polls and race analysts suggest that only three of tonight 's contests are considered competitive , all in states where incumbent Democratic governors are n't running again : Montana , New Hampshire and Washington . While those state races remain too close to call , Republicans are expected to wrest the North Carolina governorship from Democratic control , and to easily win GOP - held seats in Utah , North Dakota and Indiana . Democrats are likely hold on to their seats in West Virginia and Missouri ; and expected to notch safe wins in races for seats they hold in Vermont and Delaware . Holding Sway On Health Care While the occupant of the governor 's office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to wield significant influence in at least one major area . And that 's health care , says political scientist Thad Kousser , co - author of The Power of American Governors . \" No matter who wins the \n"
     ]
    }
   ],
   "source": [
    "!head ../SummPip/dataset/multi_news/test.truncate.fix.pun.src.txt -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7320b731-8734-4fa3-9a3d-f3c0b0ff3e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3e0ed9-ab75-4f0b-a56d-4fd13d206830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86013c39-32d3-4c44-8c01-b7ba75f41657",
   "metadata": {},
   "source": [
    "## load_documents:\n",
    "- Input:\n",
    "    - input_data: list of strings (documents) or list of file paths (strings)\n",
    "    - input_type: string ('documents' or 'file_paths')\n",
    "- Output: \n",
    "    - list of strings (documents)\n",
    "\n",
    "## build_sentence_graph:\n",
    "- Input: \n",
    "    - list of strings (documents)\n",
    "- Output: \n",
    "    - 2D list (adjacency list representing the sentence graph)\n",
    "\n",
    "## spectral_clustering:\n",
    "- Input: \n",
    "    - 2D list (adjacency list representing the sentence graph)\n",
    "- Output: \n",
    "    - list of lists, where each inner list contains spaCy sentence objects (each cluster)\n",
    "\n",
    "## fit:\n",
    "- Input: \n",
    "    - list of strings (documents)  \n",
    "    \n",
    "> No output (updates the self.clusters attribute)\n",
    "\n",
    "## compress_clusters:\n",
    "- Input: \n",
    "    - list of lists, where each inner list contains spaCy sentence objects (clusters)\n",
    "- Output: \n",
    "    - string (final summary)\n",
    "\n",
    "## transform:\n",
    "- Output: \n",
    "    - string (final summary)  \n",
    "    \n",
    "> No input (uses the self.clusters attribute)\n",
    "\n",
    "## fit_transform:\n",
    "- Input: \n",
    "    - list of strings (documents)\n",
    "    - Output: string (final summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd0115-d942-4164-8f92-71cd3092830d",
   "metadata": {},
   "source": [
    "# Helper Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b088043a-aff6-48d3-b4ab-527f8b0fd548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk.data\n",
    "\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# read source file into a list of list:\n",
    "def read_file(path, file_name, read_lead_only=False, read_first_doc=False):\n",
    "    f = open(os.path.join(path, file_name),\"r\")\n",
    "    lines = f.readlines()\n",
    "    src_list = []\n",
    "    tag=\"story_separator_special_tag\"\n",
    "    for line in lines:\n",
    "        if read_first_doc:\n",
    "            line = get_first_doc(line)\n",
    "            sent_list = truncate_doc(line)\n",
    "        elif read_lead_only:\n",
    "            sent_list = read_lead_sentences(line,tag=tag)\n",
    "        else:\n",
    "            # remove tag; uncomment below for baseline\n",
    "            line = line.replace(tag, \"\")\n",
    "            # tokenzie line to sentences\n",
    "            sent_list = sent_detector.tokenize(line.strip())\n",
    "        src_list.append(sent_list)\n",
    "    return src_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77ff9e-2942-45fc-a35f-06045f068667",
   "metadata": {},
   "source": [
    "## Sentence graph module from SummPIP Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb9f1af-4abf-4ef9-a603-b0ea8b4c5874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2020-01-16 10:17 PM\n",
    "\n",
    "author  : michelle\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from ordered_set import OrderedSet\n",
    "import scipy\n",
    "from scipy import *\n",
    "import re\n",
    "\n",
    "glove_word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "spacynlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 1: Deverbal Noun Reference\n",
    "# step 1.1: get nouns for verbs in the current sentence\n",
    "verbs_to_escape = [\"be\", \"is\",\"am\",\"are\",\"was\", \"were\", \"being\",\"been\",\"do\",\"did\",\n",
    "               \"done\",\"have\",\"had\",\"get\",\"got\",\"gotten\"]\n",
    "\n",
    "# Step 3: Discourse Markers => only for two adjacent sentences\n",
    "markers=[\"for\",\"so\",\"because\",\"since\",\"therefore\",\"consequently\",\"additionally\",\"furthermore\",\"moreover\",\n",
    "         \"but\",\"however\",\"although\",\"despite\",\"similarly\",\"otherwise\",\"whereas\",\"while\",\"unlike\",\"thus\",\n",
    "        \"instead\",\"nevertheless\",\"afterward\",\"finally\",\"subsequently\",\"conversely\",\"later\",\"next\",\"then\",\n",
    "         \"likewise\",\"compared\",\"besides\",\"further\",\"as\",\"also\",\"equally\",\"hence\",\"accordingly\",\"stil\",\n",
    "        \"simultaneously\"]\n",
    "# 39 markers\n",
    "\n",
    "class SentenceGraph:\n",
    "    def __init__(self, sentences_list, w2v, use_lm, lm_model, lm_tokenizer, ita=0.9, threshold=0.65):\n",
    "        self.sentences_list = sentences_list\n",
    "\n",
    "        self.length = len(sentences_list)\n",
    "\n",
    "        self.w2v = w2v\n",
    "\n",
    "        self.use_lm = use_lm \n",
    "\n",
    "        self.lm_model = lm_model\n",
    "\n",
    "        self.tokenizer = lm_tokenizer\n",
    "\n",
    "        # threshold for step1\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # threshold for step4\n",
    "        self.ita = ita\n",
    "\n",
    "    def get_nouns_for_verbs(self, string):\n",
    "        doc = spacynlp(string)\n",
    "        nouns_list = []\n",
    "        if len(doc)>0:\n",
    "            for token in doc:\n",
    "                # find noun reference for verbs, escaping verbs that are too ambiguous\n",
    "                if token.pos_ == \"VERB\" and token.text not in verbs_to_escape:\n",
    "                    # print(\"token.text \", token.text)\n",
    "                    noun_forms = self._nounify(token.text)\n",
    "                    nouns_list.extend(noun_forms)\n",
    "        return nouns_list\n",
    "\n",
    "    def _nounify(self, verb):\n",
    "        # get the lemmas of base verbs;\n",
    "        base = wn.morphy(verb, wn.VERB)\n",
    "        if base:\n",
    "            lemmas = wn.lemmas(base, pos=\"v\")\n",
    "            noun_forms = []\n",
    "            # derive noun forms for each lemma\n",
    "            for lemma in lemmas:\n",
    "                nouns = [forms.name() for forms in lemma.derivationally_related_forms()]\n",
    "                noun_forms.extend(nouns)\n",
    "            # remove repetition\n",
    "            nouns_set = OrderedSet(noun_forms)\n",
    "            return nouns_set\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # step 1.2: find most similar word from word2vec\n",
    "    # get most similar words for nouns, including itself\n",
    "    def find_most_similar_words(self, word_vectors,nouns_list,threshold=0.65):\n",
    "        similar_nouns_list=[]\n",
    "        nouns_list=list(set(nouns_list))\n",
    "        for noun in nouns_list:\n",
    "            try:\n",
    "                nn = word_vectors.most_similar(positive=[noun])\n",
    "                # keep nn whose have high similary score\n",
    "                nn = [ tuple_[0] for tuple_ in nn if tuple_[1] > threshold]\n",
    "                similar_nouns_list.extend(nn)\n",
    "            # pass on uncommon words\n",
    "            except KeyError:\n",
    "                pass\n",
    "        similar_nouns_list.extend(nouns_list)\n",
    "        return list(set(similar_nouns_list))\n",
    "\n",
    "    # check if deverbal noun reference exits in the subsequent sentence\n",
    "    def check_noun_reference(self, similar_nouns_list, subsequent_sen):\n",
    "        flag=False\n",
    "        doc = spacynlp(subsequent_sen)\n",
    "        if len(doc)>0:\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    if token.text in similar_nouns_list:\n",
    "                        flag=True\n",
    "                        break\n",
    "        return flag\n",
    "\n",
    "    # step 2: Event/Entity Continuation\n",
    "    # Str needs to be raw, i.e., use str before normalisation and stemming\n",
    "    def compare_name_entity(self, str1, str2):\n",
    "        flag = False\n",
    "        doc1 = spacynlp(str1)\n",
    "        doc2 = spacynlp(str2)\n",
    "        if len(doc1)>0 and len(doc2)>0:\n",
    "            ent_list1=[(ent.text, ent.label_) for ent in doc1.ents]\n",
    "            ent_list2=[(ent.text, ent.label_) for ent in doc2.ents]\n",
    "            for (text, label) in ent_list1:\n",
    "                if (text, label) in ent_list2:\n",
    "                    flag=True\n",
    "                    break\n",
    "        return flag\n",
    "\n",
    "    def check_discourse_markers(self, str1,str2):\n",
    "        flag = False\n",
    "        doc2 = spacynlp(str2)\n",
    "        if len(doc2)>0:\n",
    "            first_token = doc2[0].text\n",
    "            if first_token.lower() in markers:\n",
    "                flag = True\n",
    "        return flag\n",
    "\n",
    "    # compute the cos similarity between a and b. a, b are numpy arrays\n",
    "    def cos_sim(self, a, b):\n",
    "        return 1 - scipy.spatial.distance.cosine(a,b)\n",
    "\n",
    "\n",
    "    def make_graph_undirected(self, source, target, weight):\n",
    "        source.extend(target)\n",
    "        target.extend(source)\n",
    "        weight.extend(weight)\n",
    "        triplet_list=[ (source[i],target[i],weight[i]) for i in range(len(source))]\n",
    "        sorted_by_src = sorted(triplet_list, key=lambda x: (x[0],x[1]))\n",
    "\n",
    "        sorted_source = []\n",
    "        sorted_target = []\n",
    "        sorted_weight = []\n",
    "        for triplet in sorted_by_src:\n",
    "            sorted_source.append(triplet[0])\n",
    "            sorted_target.append(triplet[1])\n",
    "            sorted_weight.append(triplet[2])\n",
    "        return sorted_source, sorted_target, sorted_weight\n",
    "\n",
    "    # Step4: calculate sentence embeddings\n",
    "    def get_sentence_embeddings(self,string):\n",
    "        if not self.use_lm:\n",
    "            v = self.get_wv_embedding(string)\n",
    "        else:\n",
    "            v = self.get_lm_embedding(string)\n",
    "        return v\n",
    "\n",
    "    # get sentence embeddings with w2v\n",
    "    def get_wv_embedding(self, string):\n",
    "        word_embeddings = self.w2v\n",
    "        sent = string.lower()\n",
    "        eps = 1e-10\n",
    "        if len(sent) != 0:\n",
    "            vectors = [word_embeddings.get(w, np.zeros((100,))) for w in sent.split()]\n",
    "            v = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        v = v + eps\n",
    "        return v\n",
    "\n",
    "    # get language model embedding\n",
    "    def get_lm_embedding(self, string):\n",
    "        sent = string.lower()\n",
    "        eps = 1e-10\n",
    "        if len(sent)!= 0:\n",
    "            input_ids = torch.tensor([self.tokenizer.encode(sent)])\n",
    "            last_hidden_state = self.lm_model(input_ids)[0]\n",
    "            hidden_state=last_hidden_state.tolist()\n",
    "            v = np.mean(hidden_state,axis=1)\n",
    "        else:\n",
    "            v = np.zeros((768,))\n",
    "        v = v + eps\n",
    "        return v\n",
    "\n",
    "    # step 4: compare sentence similarity\n",
    "    def check_if_similar_sentences(self,sentence_emb1,sentence_emb2):\n",
    "        flag = False\n",
    "        similarity = self.cos_sim(sentence_emb1,sentence_emb2)\n",
    "        if similarity > self.ita:\n",
    "            flag = True\n",
    "        return flag\n",
    "\n",
    "\n",
    "    def build_sentence_graph(self,):\n",
    "        # spectral clustering  \n",
    "        X = np.zeros([self.length, self.length])\n",
    "        \n",
    "        # get the vector size\n",
    "        self.size = len(self.get_sentence_embeddings(self.sentences_list[0]))\n",
    "\n",
    "        # get sentence vector holder\n",
    "        emb_sentence_vectors = np.zeros([self.length,self.size])\n",
    " \n",
    "        for i in range(self.length):\n",
    "             emb_sen = self.get_sentence_embeddings(self.sentences_list[i])\n",
    "             emb_sentence_vectors[i,] = emb_sen\n",
    "        \n",
    "        # iterate all sentence nodes to check if they should be connected\n",
    "        for i in range(self.length):\n",
    "            flag = False\n",
    "            sen_i = self.sentences_list[i]\n",
    "            # check above steps\n",
    "            for j in range(i+1,self.length):\n",
    "                sen_j = self.sentences_list[j]\n",
    "                if (j-i) == 1:\n",
    "                    # perform step1 and step3,which are only for adjacent sentences\n",
    "                    nouns_list = self.get_nouns_for_verbs(sen_i)\n",
    "                    # get most similar words for above nouns, including itself\n",
    "                    similar_nouns_list = self.find_most_similar_words(glove_word_vectors, nouns_list,self.threshold)\n",
    "                    # check for devebal noun \n",
    "                    flag = self.check_noun_reference(similar_nouns_list, sen_j)\n",
    "                    if not flag:\n",
    "                        # check for disourse markers\n",
    "                        flag=self.check_discourse_markers(sen_i,sen_j)\n",
    "                else:\n",
    "                    # check for name entities\n",
    "                    flag=self.compare_name_entity(sen_i,sen_j)\n",
    "\n",
    "               # => step4 check for similar sentences\n",
    "                if not flag:\n",
    "                    # continue\n",
    "                    i_sen_emb = emb_sentence_vectors[i,]\n",
    "                    j_sen_emb = emb_sentence_vectors[j,]\n",
    "                    flag = self.check_if_similar_sentences(i_sen_emb,j_sen_emb)\n",
    "\n",
    "                if flag:                                      \n",
    "                    X[i,j] = 1\n",
    "                    X[j,i] = 1\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7eb24e55-d2c7-4710-967e-1bccaafe3b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import spacy\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.parse.earleychart import FeatureEarleyChartParser\n",
    "from tqdm import tqdm\n",
    "import configparser\n",
    "import openai\n",
    "\n",
    "class SummPIPX:\n",
    "    def __init__(self, clustering_method='spectral_clustering', graph_method='build_sentence_graph', compression_method='compress_clusters', nb_clusters=14, seed=88):\n",
    "        self.version = 'v1'\n",
    "        self.clustering_method = clustering_method\n",
    "        self.graph_method = graph_method\n",
    "        self.compression_method = compression_method\n",
    "        self.nb_clusters = nb_clusters\n",
    "        self.seed = seed\n",
    "\n",
    "    def __pdf_to_text__(self, path):\n",
    "        pdfreader = PyPDF2.PdfReader(path)\n",
    "        text=''\n",
    "        for page in pdfreader.pages:\n",
    "            text+=page.extract_text()\n",
    "        return text\n",
    "    \n",
    "    def load_documents(self, input_data, input_type='documents'):\n",
    "        # Check the input type\n",
    "        if input_type == 'documents':\n",
    "            # If input_type is 'documents', assume input_data represents the documents\n",
    "            documents = input_data\n",
    "        elif input_type == 'file_paths':\n",
    "            # If input_type is 'file_paths', assume input_data represents file paths\n",
    "            documents = [self.__pdf_to_text__(file) for file in files]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input_type: {input_type}\")\n",
    "        self.documents = documents\n",
    "\n",
    "    def build_sentence_graph(self):\n",
    "        # Default graph building method\n",
    "        nlp = spacy.load('en_core_web_md')\n",
    "        model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "        def discourse_markers(sentence1, sentence2):\n",
    "            discourse_markers = [\"accordingly\", \"additionally\", \"although\", \"as a result\", \"as an illustration\", \"as well as\", \"besides\", \"consequently\", \"conversely\", \"finally\", \"furthermore\", \"however\", \"in addition\", \"in contrast\", \"in fact\", \"in other words\", \"in short\", \"in summary\", \"indeed\", \"likewise\", \"meanwhile\", \"moreover\", \"namely\", \"nevertheless\", \"nonetheless\", \"on the contrary\", \"on the other hand\", \"otherwise\", \"overall\", \"similarly\", \"so\", \"subsequently\", \"that is\", \"then\", \"thereby\", \"therefore\", \"thus\", \"to clarify\", \"to demonstrate\", \"to elaborate\", \"to emphasize\", \"to enumerate\", \"to explain\", \"to illustrate\", \"to list\", \"to point out\", \"to put it another way\", \"to put it simply\", \"to sum up\", \"to summarize\", \"ultimately\", \"while\"]\n",
    "            pattern = r\"\\b(\" + \"|\".join(discourse_markers) + r\")\\b\"\n",
    "            if re.search(pattern, sentence2.lower()):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        def sentence_similarity(sentence1, sentence2):\n",
    "            sentence_embeddings = model.encode([sentence1, sentence2])\n",
    "            score = util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]).item()\n",
    "            return score > 0.85\n",
    "\n",
    "        def name_entity_check(sentence1, sentence2):\n",
    "            doc1 = nlp(sentence1)\n",
    "            doc2 = nlp(sentence2)\n",
    "            entities1 = set((e.text, e.label_) for e in doc1.ents)\n",
    "            entities2 = set((e.text, e.label_) for e in doc2.ents)\n",
    "            return any(entity in entities1 for entity in entities2)\n",
    "\n",
    "        def deverbal_noun(sentence1, sentence2, threshold=0.7):\n",
    "            doc1, doc2 = nlp(sentence1), nlp(sentence2)\n",
    "\n",
    "            verbs_in_sentence1 = [token.text for token in doc1 if token.pos_ == 'VERB' and token.text not in verbs_to_escape]\n",
    "\n",
    "            noun_set = set()\n",
    "            for verb in verbs_in_sentence1:\n",
    "                base_form = wn.morphy(verb, wn.VERB)\n",
    "                if base_form:\n",
    "                    lemmas = wn.lemmas(base_form, pos='v')\n",
    "                    noun_set |= set(forms.name() for lemma in lemmas for forms in lemma.derivationally_related_forms())\n",
    "            nouns_in_sentence1 = list(noun_set)\n",
    "\n",
    "            similar_nouns = set()\n",
    "            for noun in nouns_in_sentence1:\n",
    "                if noun in glove_word_vectors:\n",
    "                    similar_nouns |= set([word for word, score in glove_word_vectors.similar_by_word(noun, topn=10) if score > threshold])\n",
    "            similar_nouns= list(similar_nouns | set(nouns_in_sentence1))\n",
    "\n",
    "            check = any(token.pos_ == 'NOUN' and token.text in similar_nouns for token in doc2)\n",
    "\n",
    "            return check\n",
    "\n",
    "        sentences = []\n",
    "        for doc in self.documents:\n",
    "            sentences.extend(doc)\n",
    "\n",
    "        num_sentences = len(sentences)\n",
    "        graph = np.full((num_sentences, num_sentences), False)\n",
    "\n",
    "        for ind1, sentence1 in enumerate(tqdm(sentences)):\n",
    "            for ind2 in range(ind1 + 1, len(sentences)):\n",
    "                sentence2 = sentences[ind2]\n",
    "\n",
    "                edge = False\n",
    "                if ind1 + 1 == ind2:\n",
    "                    edge = discourse_markers(sentence1, sentence2)\n",
    "                    if not edge:\n",
    "                        edge = deverbal_noun(sentence1, sentence2)\n",
    "                if not edge:\n",
    "                    edge = name_entity_check(sentence1, sentence2)\n",
    "                if not edge:\n",
    "                    edge = sentence_similarity(sentence1, sentence2)\n",
    "\n",
    "                graph[ind1][ind2] = edge\n",
    "                graph[ind2][ind1] = edge\n",
    "\n",
    "        return graph.astype(int)\n",
    "    \n",
    "    def default_graph_method(self, documents):\n",
    "        \"\"\"\n",
    "        This is the method used in the original implementaion\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for doc in self.documents:\n",
    "            sentences.extend(doc)\n",
    "        SG = SentenceGraph(sentences, _get_w2v_embeddings('word_vec/multi_news/news_w2v.txt'), False, '', '')\n",
    "        graph = SG.build_sentence_graph()\n",
    "\n",
    "    def spectral_clustering(self, sentence_graph, documents):\n",
    "        sentences = []\n",
    "        for doc in self.documents:\n",
    "            sentences.extend(doc)\n",
    "        clustering = SpectralClustering(n_clusters = self.nb_clusters, random_state = self.seed).fit(sentence_graph)\n",
    "        clusterIDs = clustering.labels_\n",
    "\n",
    "        num_clusters = max(clusterIDs)+1\n",
    "        cluster_dict={new_list:[] for new_list in range(num_clusters)}\n",
    "\t\t# group sentences by cluster ID\n",
    "        for i, clusterID in enumerate(clusterIDs):\n",
    "            cluster_dict[clusterID].append(sentences[i])\n",
    "        return cluster_dict\n",
    "\n",
    "    def another_clustering_method(self, sentence_graph):\n",
    "        # Another clustering method\n",
    "        return []\n",
    "\n",
    "    def compress_clusters(self, clusters):\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "        summary_sentences = []\n",
    "\n",
    "        for key in clusters:\n",
    "            cluster=clusters[key]\n",
    "            if len(cluster) == 1:\n",
    "                # If there's only one sentence in the cluster, add it to the summary\n",
    "                summary_sentences.append(cluster[0])\n",
    "            else:\n",
    "                # Convert the sentences in the cluster into Spacy Doc objects\n",
    "                cluster_docs = [nlp(sent) for sent in cluster]\n",
    "\n",
    "                # Calculate the similarity scores between all sentences in the cluster\n",
    "                similarity_matrix = [[sent1.similarity(sent2) for sent2 in cluster_docs] for sent1 in cluster_docs]\n",
    "\n",
    "                # Calculate the sum of similarity scores for each sentence\n",
    "                similarity_sums = [sum(row) for row in similarity_matrix]\n",
    "\n",
    "                # Find the index of the sentence with the highest similarity score sum\n",
    "                most_relevant_index = similarity_sums.index(max(similarity_sums))\n",
    "\n",
    "                # Add the most relevant sentence to the summary\n",
    "                summary_sentences.append(cluster[most_relevant_index])\n",
    "\n",
    "        # Concatenate the summary sentences\n",
    "        summary = \" \".join(summary_sentences)\n",
    "        return summary\n",
    "\n",
    "    def gpt_compression_method(self, clusters):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config.ini')\n",
    "        API_KEY = config.get(\"openai\", \"api_key\")\n",
    "        openai.api_key = API_KEY\n",
    "        summarized_clusters = []\n",
    "\n",
    "        for key in clusters:\n",
    "            cluster = clusters[key]\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are an AI language model. Summarize the following sentences into one coherent sentence.\"}]\n",
    "            for sentence in cluster:\n",
    "                messages.append({\"role\": \"user\", \"content\": sentence})\n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages,\n",
    "                max_tokens=50,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.5,\n",
    "            )\n",
    "\n",
    "            summary = response.choices[0]['message']['content'].strip()\n",
    "            summarized_clusters.append(summary)\n",
    "\n",
    "        final_summary_messages = [{\"role\": \"system\", \"content\": \"You are an AI language model. Create a coherent summary of the following summarized sentences.\"}]\n",
    "        for summary in summarized_clusters:\n",
    "            final_summary_messages.append({\"role\": \"user\", \"content\": summary})\n",
    "\n",
    "        final_summary_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=final_summary_messages,\n",
    "            max_tokens=50,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        final_summary = final_summary_response.choices[0]['message']['content'].strip()\n",
    "        return final_summary    \n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        # Call the appropriate graph building method\n",
    "        graph_method = getattr(self, self.graph_method)\n",
    "        sentence_graph = graph_method()\n",
    "        \n",
    "        # Call the appropriate clustering method\n",
    "        clustering_method = getattr(self, self.clustering_method)\n",
    "        self.clusters = clustering_method(sentence_graph, self.documents)\n",
    "\n",
    "    def transform(self):\n",
    "        # Call the appropriate compression method\n",
    "        compression_method = getattr(self, self.compression_method)\n",
    "        summary = compression_method(self.clusters)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cc3ee-facd-4b66-b1e9-e8b606c17177",
   "metadata": {},
   "source": [
    "## Testing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae544837-bb62-4291-8390-75b2774c936e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_list = read_file('dataset/multi_news/','test.truncate.fix.pun.src.txt')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "769160ac-b9dc-41e3-a4f9-9d5bb13b1df6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:18<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "summpipx = SummPIPX()\n",
    "summpipx.load_documents(src_list)\n",
    "graph_own = summpipx.build_sentence_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8dbadf3-fb22-4ee7-b199-3a650badbb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_w2v_embeddings(w2v_file):\n",
    "    \"\"\"\n",
    "    Get w2v word embedding matrix\n",
    "\n",
    "    :return: w2v matrix\n",
    "    \"\"\"\n",
    "    word_embeddings = {}\n",
    "    f = open(w2v_file, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    return word_embeddings\n",
    "\n",
    "all_sents = src_list[0] + src_list[1]\n",
    "SG = SentenceGraph(all_sents, _get_w2v_embeddings('word_vec/multi_news/news_w2v.txt'), False, '', '')\n",
    "graph_def = SG.build_sentence_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f49d44fa-651b-4f03-9545-21195e0444f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c3689cf-56ea-4796-993f-cdf8563c7303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "862a762c-952e-4fab-8d80-6382a7f61f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1369 1313\n"
     ]
    }
   ],
   "source": [
    "comp = np.equal(graph_def, graph_own)\n",
    "print(comp.size, np.sum(comp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdce4b8-7f78-4ef5-8efe-88228df656f6",
   "metadata": {},
   "source": [
    "## We can see that we have a very similar graph to the original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d207c10-5647-49b3-8a2d-26c5c295f770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vjspranav/jupyter/Project1/summpipenv/lib/python3.10/site-packages/sklearn/cluster/_spectral.py:717: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This picture has itself caused scandal in the UK , as it was a gay kiss that was broadcast before the watershed , and as such led to a number of complaints to the BBC . Before it goes too far , I just want people to know that FB have NOT removed the kiss - in event page ; it ’s still there , but _ I made the  || News || Page 1 of 1 UPDATED : A photo of two men kissing that was posted on a Facebook page protesting a London pub ’s decision to eject a same - sex couple for kissing has been removed by the social networking site , an error , according to a rep for the company . \" GOP Eyes Gains As Voters In 11 States Pick Governors Enlarge this image toggle caption Jim Cole / AP Jim Cole / AP Voters in 11 states will pick their governors tonight , and Republicans appear on track to increase their numbers by at least one , with the potential to extend their hold to more than two - thirds of the nation \\'s top state offices . UPDATE : 4/19/2001 Read Richard Metzger : How I , a married , middle - aged man , became an accidental spokesperson for gay rights overnight on Boing Boing It ’s time to clarify a few details about the controversial “ Hey Facebook what ’s SO wrong with a pic of two men kissing ? Eight of the gubernatorial seats up for grabs today are now held by Democrats ; three are in Republican hands . Republicans currently hold 29 governorships , Democrats have 20 , and Rhode Island \\'s Gov. Democrats are likely to hold on to their seats in West Virginia and Missouri , and are expected to notch safe wins in races for seats they hold in Vermont and Delaware . Polls and race analysts suggest that only three of tonight \\'s contests are considered competitive , all in states where incumbent Democratic governors are n\\'t running again : Montana , New Hampshire and Washington . Eight of the gubernatorial seats up for grabs are now held by Democrats ; three are in Republican hands . While those state races remain too close to call , Republicans are expected to wrest the North Carolina governorship from Democratic control , and to easily win GOP - held seats in Utah , North Dakota and Indiana . Republicans currently hold 29 governorships , Democrats have 20 ; and Rhode Island \\'s Gov. No matter who wins the  GOP Eyes Gains As Voters In 11 States Pick Governors Jim Cole / AP i Jim Cole / AP Voters in 11 states will pick their governors tonight , and Republicans appear on track to increase their numbers by at least one , and with the potential to extend their hold to more than two - thirds of the nation \\'s top state offices . Holding Sway On Health Care While the occupant of the governor \\'s office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to wield significant influence in at least one major area . First of all , with regards to the picture : The photo which was used to illustrate my first post about the John Snow Kiss - In is a promotional still from the British soap opera “ Eastenders .'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters=summpipx.spectral_clustering(graph_def, all_sents)\n",
    "summpipx.compress_clusters(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db303b-38f7-4387-997e-0161aeb05643",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6be04211-5084-4800-ab25-eb74d778fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:18<00:00,  2.03it/s]\n",
      "/home/vjspranav/jupyter/Project1/summpipenv/lib/python3.10/site-packages/sklearn/cluster/_spectral.py:717: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No matter who wins the While those state races remain too close to call , Republicans are expected to wrest the North Carolina governorship from Democratic control , and to easily win GOP - held seats in Utah , North Dakota and Indiana . This picture has itself caused scandal in the UK , as it was a gay kiss that was broadcast before the watershed , and as such led to a number of complaints to the BBC . Before it goes too far , I just want people to know that FB have NOT removed the kiss - in event page ; it ’s still there , but _ I made the  || News || Page 1 of 1 UPDATED : A photo of two men kissing that was posted on a Facebook page protesting a London pub ’s decision to eject a same - sex couple for kissing has been removed by the social networking site , an error , according to a rep for the company . \" Eight of the gubernatorial seats up for grabs today are now held by Democrats ; three are in Republican hands . Republicans currently hold 29 governorships , Democrats have 20 , and Rhode Island \\'s Gov. GOP Eyes Gains As Voters In 11 States Pick Governors Enlarge this image toggle caption Jim Cole / AP Jim Cole / AP Voters in 11 states will pick their governors tonight , and Republicans appear on track to increase their numbers by at least one , with the potential to extend their hold to more than two - thirds of the nation \\'s top state offices . Holding Sway On Health Care While the occupant of the governor \\'s office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to wield significant influence in at least one major area . Polls and race analysts suggest that only three of tonight \\'s contests are considered competitive , all in states where incumbent Democratic governors are n\\'t running again : Montana , New Hampshire and Washington . Secondly , the removal of the Facebook John Snow Kiss - In event : It turns out that the Facebook event for the John Snow Kiss - In was not blocked by Facebook , but made private by the creator of the event itself . First of all , with regards to the picture : The photo which was used to illustrate my first post about the John Snow Kiss - In is a promotional still from the British soap opera “ Eastenders . ” story , as it now beginning to be reported in the mainstream media , and not always correctly . UPDATE : 4/19/2001 Read Richard Metzger : How I , a married , middle - aged man , became an accidental spokesperson for gay rights overnight on Boing Boing It ’s time to clarify a few details about the controversial “ Hey Facebook what ’s SO wrong with a pic of two men kissing ? According to NYULocal.com , the photo was quickly removed and the following e - mail was sent to administrators of the Facebook page : “ Shares that contain nudity , or any kind of graphic or sexually suggestive content , are not permitted on Facebook .'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summpipx = SummPIPX()\n",
    "summpipx.load_documents(src_list)\n",
    "summpipx.fit()\n",
    "summpipx.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b15d76d-1368-470a-a2d0-a06725cacddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:18<00:00,  2.01it/s]\n",
      "/home/vjspranav/jupyter/Project1/summpipenv/lib/python3.10/site-packages/sklearn/cluster/_spectral.py:717: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NYULocal.com reported that a photo was removed from a Facebook page promoting a \"gay kiss-in\" demonstration in London, and administrators were sent an email stating that nudity and sexually suggestive content is not allowed on the platform. However, the photo was later clarified to not have been removed and was instead made private by the event creator. The photo used in the post was a promotional still from the British soap opera \"Eastenders\" and was chosen because it was considered mild. Meanwhile, in the current gubernatorial election, Republicans are predicted to take control of the North Carolina governorship from the Democrats and win GOP-held seats in Utah, North Dakota, and Indiana, despite the uncertainty surrounding the other state races.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summpipxgpt = SummPIPX(compression_method='gpt_compression_method')\n",
    "summpipxgpt.load_documents(src_list)\n",
    "summpipxgpt.fit()\n",
    "summpipxgpt.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7054e26-d361-44fb-b64d-cc82e8caf19e",
   "metadata": {},
   "source": [
    "# The above summary is generated with 500 tokens and in code it has been hardcoded to 50 to ensure that uneeded tokens don't get used up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e76f0c-2b84-4a63-af51-baab93caf1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SummPIP-Kernel",
   "language": "python",
   "name": "summpip-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
