{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd3bc65-b8d8-4636-beba-10f84b4330a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in /home/vjspranav/.local/lib/python3.10/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7090b466-9cb7-480a-9b39-b840d6ad96ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7c20323-5047-44de-b113-6eb47e62e8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(path):\n",
    "    pdfreader = PyPDF2.PdfReader(path)\n",
    "    text=''\n",
    "    for page in pdfreader.pages:\n",
    "        text+=page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaedd4a-063a-446d-80c6-b7eaac235312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/45605946/how-to-do-text-pre-processing-using-spacy\n",
    "\n",
    "import spacy #load spacy\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "Data['Text_After_Clean'] = Data['Text'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b9ccc32-f165-4c53-99fa-cbf8b4ad5d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Convert to lowercase\n",
    "    # words = [word.lower() for word in words]\n",
    "\n",
    "    # Remove punctuations and non-alphabetic characters\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "\n",
    "    # Stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the cleaned words back into a text string\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    # Return the cleaned text\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d25406cb-863a-416f-b477-b9bf06b7d24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    'dataset/NeuralNetworks/1460210.pdf', \n",
    "    'dataset/NeuralNetworks/Oken.pdf',\n",
    "    'dataset/NeuralNetworks/week7b-neuralnetwork.pdf'\n",
    "]\n",
    "\n",
    "pdftexts = [pdf_to_text(file) for file in files]\n",
    "\n",
    "with open('dataset/NeuralNetworks/mergeddoc.txt', 'w') as f:\n",
    "    for pdftext in pdftexts:\n",
    "        f.write(pdftext)\n",
    "cleanpdftexts = [clean_text(text) for text in pdftexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603156d-a9e0-4c96-8d41-2c2041d62e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c97118-c641-478e-b628-4e20df36edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SummPIP-Kernel",
   "language": "python",
   "name": "summpip-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
